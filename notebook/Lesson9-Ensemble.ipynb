{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from dataset import MaskBaseDataset\n",
    "from model import *\n",
    "from loss import create_criterion\n",
    "\n",
    "sys.path.append('../')\n",
    "from dataset import MaskMultiClassDataset\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        seed: seed 정수값\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9 - Ensemble\n",
    "- 이번 실습 자료에서는 강의시간에 다루었던 Straified kFold Cross Validation(교차 검증)에 대해 다뤄보겠습니다. k개의 Fold로 나누어 교차 검증하는 kFold Cross Validation은 기존 train, valid 로 나누어 한번만 검증하던 프로세스에서 조금 더 엄격하게 모델을 검증하는 방법입니다. \n",
    "- Sklearn의 kFold는 데이터셋의 인덱스로 Fold를 정의하므로 기존에 사용하던 DataSet이 아닌 인덱스로 생성한 Subset을 사용합니다. 이번 실습 자료에서는 매 이터레이션마다 생성한 SubSet 객체를 활용해 DataLoader를 반환해 학습 및 검증에 사용하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- parameters\n",
    "img_root = '/mnt/ssd/data/mask_final/train/images'  # 학습 이미지 폴더의 경로\n",
    "label_path = '/mnt/ssd/data/mask_final/train/train.csv'  # 학습 메타파일의 경로\n",
    "\n",
    "model_name = \"VGG19\"  # 모델 이름\n",
    "use_pretrained = True  # pretrained-model의 사용 여부\n",
    "freeze_backbone = False  # classifier head 이 외 부분을 업데이트되지 않게 할 것인지 여부\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "num_classes = 18\n",
    "\n",
    "num_epochs = 100  # 학습할 epoch의 수\n",
    "lr = 1e-4\n",
    "lr_decay_step = 10\n",
    "criterion_name = 'cross_entropy' # loss의 이름\n",
    "\n",
    "train_log_interval = 20  # logging할 iteration의 주기\n",
    "name = \"02_vgg\"  # 결과를 저장하는 폴더의 이름\n",
    "\n",
    "# -- settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "- index를 사용한 Dataloader 정의\n",
    "- getDataloader 함수 설명\n",
    "    1. Pytorch Dataset, train 인덱스, valid 인덱스, batch size를 전달받아 Train, Valid DataLoader 객체를 반환합니다.\n",
    "    2. torch.utils.data.Subset 객체는 데이터셋과 해당 데이터셋의 인덱스를 전달받아 Subset 객체를 생성합니다. 생성한 Subset 객체를 사용해 DataLoader 객체를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataloader(dataset, train_idx, valid_idx, batch_size, num_workers):\n",
    "    train_set = torch.utils.data.Subset(dataset,\n",
    "                                        indices=train_idx)\n",
    "    val_set   = torch.utils.data.Subset(dataset,\n",
    "                                        indices=valid_idx)\n",
    "    val_set.dataset.set_phase(\"test\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified k-Fold\n",
    "1. k를 나타내는 n_splits 값을 설정해 StratifiedKFold 객체를 준비합니다. \n",
    "2. skf.split(x, y) 메소드를 사용해 데이터셋의 인덱스를 얻어내고, 라벨(y)에 해당하는 dataset.labels를 기준으로 Stratified를 진행합니다. \n",
    "3. 매 이터레이션마다 반환받은 인덱스를 사용해 getDataloader 함수에 전달하여 DataLoader를 생성합니다.\n",
    "4. 나머지 학습 프로세스는 이전 강의에서 사용한 프로세스와 동일하게 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaskMultiClassDataset(img_root, label_path, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "counter = 0\n",
    "patience = 10\n",
    "accumulation_steps = 2\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(dataset.image_paths, dataset.labels)):\n",
    "    train_loader, val_loader = getDataloader(dataset, train_idx, valid_idx, batch_size, num_workers)\n",
    "\n",
    "    # -- model\n",
    "    if model_name == \"AlexNet\":\n",
    "        model = AlexNet(num_classes=num_classes, pretrained=use_pretrained, freeze=freeze_backbone).to(device)\n",
    "    else:\n",
    "        model = VGG19(num_classes=num_classes, pretrained=use_pretrained, freeze=freeze_backbone).to(device)\n",
    "\n",
    "    # -- loss & metric\n",
    "    criterion = create_criterion(criterion_name)\n",
    "    train_params = [{'params': getattr(model.net, 'features').parameters(), 'lr': lr / 10, 'weight_decay':5e-4},\n",
    "                    {'params': getattr(model.net, 'classifier').parameters(), 'lr': lr, 'weight_decay':5e-4}]\n",
    "    optimizer = Adam(train_params)\n",
    "    scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)\n",
    "\n",
    "    # -- logging\n",
    "    logger = SummaryWriter(log_dir=f\"results/cv{i}_{name}\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        loss_value = 0\n",
    "        matches = 0\n",
    "        for idx, train_batch in enumerate(train_loader):\n",
    "            inputs, labels = train_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = criterion(outs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "             # -- Gradient Accumulation\n",
    "            if (idx+1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "            matches += (preds == labels).sum().item()\n",
    "            if (idx + 1) % train_log_interval == 0:\n",
    "                train_loss = loss_value / train_log_interval\n",
    "                train_acc = matches / batch_size / train_log_interval\n",
    "                current_lr = scheduler.get_last_lr()\n",
    "                print(\n",
    "                    f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                    f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "                )\n",
    "\n",
    "                loss_value = 0\n",
    "                matches = 0\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # val loop\n",
    "        with torch.no_grad():\n",
    "            print(\"Calculating validation results...\")\n",
    "            model.eval()\n",
    "            val_loss_items = []\n",
    "            val_acc_items = []\n",
    "            for val_batch in val_loader:\n",
    "                inputs, labels = val_batch\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outs = model(inputs)\n",
    "                preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "                loss_item = criterion(outs, labels).item()\n",
    "                acc_item = (labels == preds).sum().item()\n",
    "                val_loss_items.append(loss_item)\n",
    "                val_acc_items.append(acc_item)\n",
    "\n",
    "            val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "            val_acc = np.sum(val_acc_items) / len(valid_idx)\n",
    "\n",
    "            # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "            if val_acc > best_val_acc:\n",
    "                print(\"New best model for val accuracy! saving the model..\")\n",
    "                torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "                best_val_acc = val_acc\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "            # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "            if counter > patience:\n",
    "                print(\"Early Stopping...\")\n",
    "                break\n",
    "\n",
    "\n",
    "            print(\n",
    "                f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "                f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle",
   "language": "python",
   "name": "kyle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
