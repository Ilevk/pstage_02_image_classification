{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "knowing-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os, sys\n",
    "from importlib import import_module\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from dataset import MaskBaseDataset\n",
    "from model import *\n",
    "\n",
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "working-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- parameters\n",
    "img_root = '/mnt/ssd/data/mask/resized_data/'\n",
    "label_path = '/mnt/ssd/data/mask/metadata.csv'\n",
    "\n",
    "model_name = \"VGG19\"\n",
    "use_pretrained = True\n",
    "freeze_backbone = False\n",
    "\n",
    "val_split = 0.4\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "num_classes = 3\n",
    "\n",
    "num_epochs = 100\n",
    "lr = 1e-4\n",
    "lr_decay_step = 10\n",
    "\n",
    "train_log_interval = 20\n",
    "name = \"02_vgg\"\n",
    "\n",
    "# -- settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-vulnerability",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-example",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ranging-profession",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-offense",
   "metadata": {},
   "source": [
    "### Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "english-camel",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-airfare",
   "metadata": {},
   "source": [
    "### Label Smoothing Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daily-requirement",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-ontario",
   "metadata": {},
   "source": [
    "### F1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eastern-nudist",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n",
    "class F1Loss(nn.Module):\n",
    "    def __init__(self, classes=3, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.ndim == 2\n",
    "        assert y_true.ndim == 1\n",
    "        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n",
    "        return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "resistant-nudist",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-nutrition",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fifty-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- model\n",
    "model_cls = getattr(import_module(\"model\"), model_name)\n",
    "model = model_cls(\n",
    "    num_classes=num_classes,\n",
    "    pretrained=use_pretrained,\n",
    "    freeze=freeze_backbone\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "convenient-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SGD optimizer\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opened-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Adam optimizer\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dental-catalog",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('net',\n",
       "  VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): ReLU(inplace=True)\n",
       "      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (19): ReLU(inplace=True)\n",
       "      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (32): ReLU(inplace=True)\n",
       "      (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (38): ReLU(inplace=True)\n",
       "      (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (42): ReLU(inplace=True)\n",
       "      (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (45): ReLU(inplace=True)\n",
       "      (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (48): ReLU(inplace=True)\n",
       "      (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (51): ReLU(inplace=True)\n",
       "      (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=3, bias=True)\n",
       "    )\n",
       "  ))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "transsexual-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- optimizer: Different Learning Rates on different layers\n",
    "\n",
    "train_params = [{'params': getattr(model.net, 'features').parameters(), 'lr': lr / 10, 'weight_decay':5e-4},\n",
    "                {'params': getattr(model.net, 'classifier').parameters(), 'lr': lr, 'weight_decay':5e-4}]\n",
    "optimizer = Adam(train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-comment",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "directed-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scheduler: StepLR\n",
    "\n",
    "scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "golden-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scheduler: ReduceLROnPlateau\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "massive-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scheduler: CosineAnnealingLR\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-appraisal",
   "metadata": {},
   "source": [
    "## Metric\n",
    "Example Code is from scikit-learn tutorial codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "prospective-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-bosnia",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "functioning-austin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reduced-submission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-fleet",
   "metadata": {},
   "source": [
    "### Precision & Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "human-stanley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fantastic-gardening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-keeping",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "liable-paper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "amazing-invite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-pittsburgh",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "closing-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaskBaseDataset(img_root, label_path, 'train')\n",
    "n_val = int(len(dataset) * val_split)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "val_set.dataset.set_phase(\"test\")  # todo : fix\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-behalf",
   "metadata": {},
   "source": [
    "### Callback - Checkpoint, Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "norwegian-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Callback1: Save Checkpoints by Accuracy\n",
    "# Continue to the training code\n",
    "\n",
    "# -- Callback2: Early Stopping\n",
    "patience = 10\n",
    "counter = 0\n",
    "# Continue to the training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-reference",
   "metadata": {},
   "source": [
    "### Training Method - Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "remarkable-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Gradient Accumulation\n",
    "accumulation_steps = 2\n",
    "# Continue to the training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-graduate",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-matthew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/100](20/287) || training loss 0.6277 || training accuracy 72.81% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](40/287) || training loss 0.2611 || training accuracy 91.02% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](60/287) || training loss 0.1992 || training accuracy 93.28% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](80/287) || training loss 0.1265 || training accuracy 96.17% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](100/287) || training loss 0.1084 || training accuracy 96.88% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](120/287) || training loss 0.09347 || training accuracy 96.95% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](140/287) || training loss 0.0781 || training accuracy 97.19% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](160/287) || training loss 0.1003 || training accuracy 96.88% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](180/287) || training loss 0.05327 || training accuracy 98.20% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](200/287) || training loss 0.0734 || training accuracy 97.58% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](220/287) || training loss 0.04078 || training accuracy 98.59% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](240/287) || training loss 0.06054 || training accuracy 98.28% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](260/287) || training loss 0.05324 || training accuracy 98.12% || lr [1e-05, 0.0001]\n",
      "Epoch[0/100](280/287) || training loss 0.04866 || training accuracy 98.67% || lr [1e-05, 0.0001]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 98.33%, loss: 0.046 || best acc : 98.33%, best loss: 0.046\n",
      "Epoch[1/100](20/287) || training loss 0.04544 || training accuracy 98.67% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](40/287) || training loss 0.02798 || training accuracy 99.22% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](60/287) || training loss 0.02541 || training accuracy 99.45% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](80/287) || training loss 0.02213 || training accuracy 99.45% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](100/287) || training loss 0.02032 || training accuracy 99.45% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](120/287) || training loss 0.01783 || training accuracy 99.38% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](140/287) || training loss 0.009559 || training accuracy 99.84% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](160/287) || training loss 0.01649 || training accuracy 99.53% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](180/287) || training loss 0.01635 || training accuracy 99.22% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](200/287) || training loss 0.01426 || training accuracy 99.53% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](220/287) || training loss 0.004601 || training accuracy 99.92% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](240/287) || training loss 0.006294 || training accuracy 99.84% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](260/287) || training loss 0.003547 || training accuracy 100.00% || lr [5e-06, 5e-05]\n",
      "Epoch[1/100](280/287) || training loss 0.01388 || training accuracy 99.77% || lr [5e-06, 5e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 98.72%, loss: 0.043 || best acc : 98.72%, best loss: 0.043\n",
      "Epoch[2/100](20/287) || training loss 0.01584 || training accuracy 99.61% || lr [0.0, 0.0]\n",
      "Epoch[2/100](40/287) || training loss 0.007448 || training accuracy 99.61% || lr [0.0, 0.0]\n",
      "Epoch[2/100](60/287) || training loss 0.009016 || training accuracy 99.77% || lr [0.0, 0.0]\n",
      "Epoch[2/100](80/287) || training loss 0.006454 || training accuracy 99.77% || lr [0.0, 0.0]\n",
      "Epoch[2/100](100/287) || training loss 0.01047 || training accuracy 99.61% || lr [0.0, 0.0]\n",
      "Epoch[2/100](120/287) || training loss 0.007836 || training accuracy 99.69% || lr [0.0, 0.0]\n",
      "Epoch[2/100](140/287) || training loss 0.002867 || training accuracy 99.92% || lr [0.0, 0.0]\n",
      "Epoch[2/100](160/287) || training loss 0.006628 || training accuracy 99.77% || lr [0.0, 0.0]\n",
      "Epoch[2/100](180/287) || training loss 0.001769 || training accuracy 100.00% || lr [0.0, 0.0]\n",
      "Epoch[2/100](200/287) || training loss 0.004768 || training accuracy 99.92% || lr [0.0, 0.0]\n",
      "Epoch[2/100](220/287) || training loss 0.002677 || training accuracy 99.92% || lr [0.0, 0.0]\n",
      "Epoch[2/100](240/287) || training loss 0.004436 || training accuracy 99.84% || lr [0.0, 0.0]\n",
      "Epoch[2/100](260/287) || training loss 0.002443 || training accuracy 100.00% || lr [0.0, 0.0]\n",
      "Epoch[2/100](280/287) || training loss 0.006143 || training accuracy 99.77% || lr [0.0, 0.0]\n",
      "Calculating validation results...\n",
      "[Val] acc : 98.71%, loss: 0.043 || best acc : 98.72%, best loss: 0.043\n",
      "Epoch[3/100](20/287) || training loss 0.01519 || training accuracy 99.53% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](40/287) || training loss 0.007961 || training accuracy 99.77% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](60/287) || training loss 0.007474 || training accuracy 99.77% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](80/287) || training loss 0.007574 || training accuracy 99.92% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](100/287) || training loss 0.01277 || training accuracy 99.53% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](120/287) || training loss 0.004494 || training accuracy 99.92% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](140/287) || training loss 0.002835 || training accuracy 100.00% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](160/287) || training loss 0.006093 || training accuracy 99.77% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](180/287) || training loss 0.005226 || training accuracy 99.92% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](200/287) || training loss 0.004298 || training accuracy 99.92% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](220/287) || training loss 0.002104 || training accuracy 99.92% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](240/287) || training loss 0.004027 || training accuracy 99.92% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](260/287) || training loss 0.00153 || training accuracy 100.00% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/100](280/287) || training loss 0.002119 || training accuracy 99.92% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 98.79%, loss: 0.05 || best acc : 98.79%, best loss: 0.043\n",
      "Epoch[4/100](20/287) || training loss 0.002295 || training accuracy 99.92% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](40/287) || training loss 0.000873 || training accuracy 100.00% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](60/287) || training loss 0.001604 || training accuracy 100.00% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](80/287) || training loss 0.001776 || training accuracy 99.92% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](100/287) || training loss 0.005652 || training accuracy 99.92% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](120/287) || training loss 0.002169 || training accuracy 99.92% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](140/287) || training loss 0.0007637 || training accuracy 100.00% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](160/287) || training loss 0.002715 || training accuracy 99.84% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](180/287) || training loss 0.001469 || training accuracy 100.00% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](200/287) || training loss 0.002633 || training accuracy 99.92% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](220/287) || training loss 0.0005358 || training accuracy 100.00% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](240/287) || training loss 0.001788 || training accuracy 99.92% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](260/287) || training loss 0.0008746 || training accuracy 100.00% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/100](280/287) || training loss 0.001626 || training accuracy 99.84% || lr [1e-05, 0.00010000000000000002]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "counter = 0\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    # train loop\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for idx, train_batch in enumerate(train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outs = model(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # -- Gradient Accumulation\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % train_log_interval == 0:\n",
    "            train_loss = loss_value / train_log_interval\n",
    "            train_acc = matches / batch_size / train_log_interval\n",
    "            current_lr = scheduler.get_last_lr()\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"Calculating validation results...\")\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(val_set)\n",
    "        \n",
    "        # Callback1\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if val_acc > best_val_acc:\n",
    "            print(\"New best model for val accuracy! saving the model..\")\n",
    "            torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # Callback2\n",
    "        if counter > patience:\n",
    "            print(\"Early Stopping...\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-dylan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
